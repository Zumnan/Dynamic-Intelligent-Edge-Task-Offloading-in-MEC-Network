{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3b7d18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Location Area Code(LAC)  Cell Identity(CI)                    End Time  \\\n",
      "0                     3288              11803  2015-08-01 00:08:19.952078   \n",
      "1                    43023              20723  2015-08-01 00:08:35.534758   \n",
      "2                     3288              11803  2015-08-01 00:08:20.218324   \n",
      "3                     3287               7483  2015-08-01 00:08:21.168647   \n",
      "4                    43023              20723  2015-08-01 00:08:41.388309   \n",
      "\n",
      "   Duration  Uplink traffic  Downlink traffic  Radio Access Type(RAT)  \\\n",
      "0         2             585               464                       2   \n",
      "1        15             300                 0                       1   \n",
      "2         1             559               514                       2   \n",
      "3        82             559               900                       2   \n",
      "4      2131            2280              1276                       1   \n",
      "\n",
      "        Source IP  Destination IP  \\\n",
      "0    172.21.68.99    112.90.17.54   \n",
      "1  172.25.166.212  243.185.187.39   \n",
      "2    172.21.68.99  58.254.134.137   \n",
      "3     10.90.52.80      10.0.0.172   \n",
      "4  172.25.166.212    110.76.19.13   \n",
      "\n",
      "                                                 URL    User Agent  \\\n",
      "0  http://pgv.m.xunlei.com/?u=pgv_base_action_100...           NaN   \n",
      "1                                                NaN           NaN   \n",
      "2                 http://hub5pr.wap.sandai.net:3076/           NaN   \n",
      "3  http://140.205.203.103:8026/m/23047187/45/AkBv...  Agoo-sdk-2.0   \n",
      "4                                                NaN           NaN   \n",
      "\n",
      "   Start Date  Start Hour  \n",
      "0  2015-08-01           0  \n",
      "1  2015-08-01           0  \n",
      "2  2015-08-01           0  \n",
      "3  2015-08-01           0  \n",
      "4  2015-07-31          23  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the path to the dataset\n",
    "dataset_path = r'C:\\Users\\Administrator\\Downloads\\Edge-Computing-Dataset-master\\Edge-Computing-Dataset-master\\Data'\n",
    "\n",
    "# Define a list to store the preprocessed data\n",
    "preprocessed_data = []\n",
    "\n",
    "# Iterate over the subfolders\n",
    "for folder in ['shanghai', 'guangzhou']:\n",
    "    folder_path = os.path.join(dataset_path, folder)\n",
    "    \n",
    "    # Iterate over the CSV files in the subfolder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            \n",
    "            # Load the CSV file into a DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Perform data cleaning and filtering\n",
    "            df = df[['Location Area Code(LAC)', 'Cell Identity(CI)', 'Start Time', 'End Time',\n",
    "                     'Duration', 'Uplink traffic', 'Downlink traffic', 'Radio Access Type(RAT)',\n",
    "                     'Source IP', 'Destination IP', 'URL', 'User Agent']]\n",
    "            \n",
    "            # Apply any additional preprocessing steps if required\n",
    "            # Extract date and time information from 'Start Time'\n",
    "            df['Start Date'] = pd.to_datetime(df['Start Time']).dt.date\n",
    "            df['Start Hour'] = pd.to_datetime(df['Start Time']).dt.hour\n",
    "            \n",
    "            # Drop the original 'Start Time' column\n",
    "            df.drop('Start Time', axis=1, inplace=True)\n",
    "            \n",
    "            # Append the preprocessed data to the list\n",
    "            preprocessed_data.append(df)\n",
    "\n",
    "# Concatenate all the preprocessed dataframes into a single dataframe\n",
    "preprocessed_df = pd.concat(preprocessed_data, ignore_index=True)\n",
    "\n",
    "# Print the preprocessed dataframe\n",
    "print(preprocessed_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb957f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Most Frequently Accessed Content:\n",
      "http://szextshort.weixin.qq.com/cgi-bin/micromsg-bin/mmsnssync         2528\n",
      "http://monitor.uu.qq.com/analytics/upload                              1034\n",
      "http://short.weixin.qq.com/cgi-bin/micromsg-bin/mmsnssync               911\n",
      "http://loc.map.baidu.com/sdk.php                                        674\n",
      "http://szextshort.weixin.qq.com/cgi-bin/micromsg-bin/getpackagelist     495\n",
      "http://szextshort.weixin.qq.com/cgi-bin/micromsg-bin/mmsnstimeline      451\n",
      "http://strategy.beacon.qq.com/analytics/upload?mType=beacon             429\n",
      "http://szminorshort.weixin.qq.com/cgi-bin/micromsg-bin/reportkvcomm     402\n",
      "http://szshort.weixin.qq.com/cgi-bin/micromsg-bin/newsync               393\n",
      "http://szextshort.weixin.qq.com/cgi-bin/micromsg-bin/mmsnsuserpage      386\n",
      "Name: URL, dtype: int64\n",
      "\n",
      "Distribution of User Requests across Base Stations:\n",
      "Location Area Code(LAC)  Cell Identity(CI)\n",
      "2180                     16651                 4\n",
      "                         38571                 1\n",
      "                         52733                 7\n",
      "                         56491                 4\n",
      "2181                     15062                 1\n",
      "                                              ..\n",
      "53533                    16161                14\n",
      "                         17541                 1\n",
      "53537                    12522                 6\n",
      "                         13702                39\n",
      "57699                    26826                 1\n",
      "Length: 8230, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the path to the dataset\n",
    "dataset_path = r'C:\\Users\\Administrator\\Downloads\\Edge-Computing-Dataset-master\\Edge-Computing-Dataset-master\\Data'\n",
    "\n",
    "# Define a list to store the preprocessed data\n",
    "preprocessed_data = []\n",
    "\n",
    "# Iterate over the subfolders\n",
    "for folder in ['shanghai', 'guangzhou']:\n",
    "    folder_path = os.path.join(dataset_path, folder)\n",
    "    \n",
    "    # Iterate over the CSV files in the subfolder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            \n",
    "            # Load the CSV file into a DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Perform data cleaning and filtering\n",
    "            df = df[['Location Area Code(LAC)', 'Cell Identity(CI)', 'Start Time', 'End Time',\n",
    "                     'Duration', 'Uplink traffic', 'Downlink traffic', 'Radio Access Type(RAT)',\n",
    "                     'Source IP', 'Destination IP', 'URL', 'User Agent']]\n",
    "            \n",
    "            # Apply any additional preprocessing steps if required\n",
    "            \n",
    "            # Append the preprocessed data to the list\n",
    "            preprocessed_data.append(df)\n",
    "\n",
    "# Concatenate all the preprocessed dataframes into a single dataframe\n",
    "preprocessed_df = pd.concat(preprocessed_data, ignore_index=True)\n",
    "\n",
    "# Analyze content popularity\n",
    "content_popularity = preprocessed_df['URL'].value_counts()\n",
    "\n",
    "# Print the most frequently accessed content\n",
    "print(\"Top 10 Most Frequently Accessed Content:\")\n",
    "print(content_popularity.head(10))\n",
    "\n",
    "# Calculate the distribution of user requests across base stations\n",
    "base_station_distribution = preprocessed_df.groupby(['Location Area Code(LAC)', 'Cell Identity(CI)']).size()\n",
    "\n",
    "# Print the distribution of user requests across base stations\n",
    "print(\"\\nDistribution of User Requests across Base Stations:\")\n",
    "print(base_station_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57312b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache Contents:\n",
      "['http://example.com/content1']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Define the path to the dataset\n",
    "dataset_path = r'C:\\Users\\Administrator\\Downloads\\Edge-Computing-Dataset-master\\Edge-Computing-Dataset-master\\Data'\n",
    "\n",
    "# Define a list to store the preprocessed data\n",
    "preprocessed_data = []\n",
    "\n",
    "# Iterate over the subfolders\n",
    "for folder in ['shanghai', 'guangzhou']:\n",
    "    folder_path = os.path.join(dataset_path, folder)\n",
    "    \n",
    "    # Iterate over the CSV files in the subfolder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            \n",
    "            # Load the CSV file into a DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Perform data cleaning and filtering\n",
    "            df = df[['Location Area Code(LAC)', 'Cell Identity(CI)', 'Start Time', 'End Time',\n",
    "                     'Duration', 'Uplink traffic', 'Downlink traffic', 'Radio Access Type(RAT)',\n",
    "                     'Source IP', 'Destination IP', 'URL', 'User Agent']]\n",
    "            \n",
    "            # Apply any additional preprocessing steps if required\n",
    "            \n",
    "            # Append the preprocessed data to the list\n",
    "            preprocessed_data.append(df)\n",
    "\n",
    "# Concatenate all the preprocessed dataframes into a single dataframe\n",
    "preprocessed_df = pd.concat(preprocessed_data, ignore_index=True)\n",
    "\n",
    "# Analyze content popularity\n",
    "content_popularity = preprocessed_df['URL'].value_counts()\n",
    "\n",
    "# Design caching policy using LRU algorithm\n",
    "cache_size = 1000  # Specify the cache size\n",
    "\n",
    "# Sort the content popularity in descending order\n",
    "sorted_content_popularity = content_popularity.sort_values(ascending=False)\n",
    "\n",
    "# Initialize the cache as an ordered dictionary with a limited size\n",
    "cache = OrderedDict()\n",
    "\n",
    "# Function to check if a content is in the cache\n",
    "def in_cache(content):\n",
    "    return content in cache\n",
    "\n",
    "# Function to access a content (update the LRU order)\n",
    "def access_content(content):\n",
    "    if in_cache(content):\n",
    "        # Move the accessed content to the end to update its recency\n",
    "        cache.move_to_end(content)\n",
    "\n",
    "# Function to add a content to the cache (evict the least recently used content if the cache is full)\n",
    "def add_content_to_cache(content):\n",
    "    if not in_cache(content):\n",
    "        if len(cache) >= cache_size:\n",
    "            # Remove the least recently used content\n",
    "            cache.popitem(last=False)\n",
    "        cache[content] = None\n",
    "        # Move the newly added content to the end to mark it as the most recently used\n",
    "\n",
    "# Example usage of the caching policy\n",
    "# Suppose we want to access a content\n",
    "content_to_access = 'http://example.com/content1'\n",
    "access_content(content_to_access)\n",
    "\n",
    "# If the content is not in the cache, add it to the cache\n",
    "if not in_cache(content_to_access):\n",
    "    add_content_to_cache(content_to_access)\n",
    "\n",
    "# Print the cache contents (most recently used content at the end)\n",
    "print(\"Cache Contents:\")\n",
    "print(list(cache.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08a1eaca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Hit Ratio: 0.7624485531410047\n",
      "Latency Reduction: 0.7624485531410047\n",
      "Traffic Offloaded: 1629248642\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Define the path to the dataset\n",
    "dataset_path = r'C:\\Users\\Administrator\\Downloads\\Edge-Computing-Dataset-master\\Edge-Computing-Dataset-master\\Data'\n",
    "\n",
    "# Define a list to store the preprocessed data\n",
    "preprocessed_data = []\n",
    "\n",
    "# Iterate over the subfolders\n",
    "for folder in ['shanghai', 'guangzhou']:\n",
    "    folder_path = os.path.join(dataset_path, folder)\n",
    "    \n",
    "    # Iterate over the CSV files in the subfolder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            \n",
    "            # Load the CSV file into a DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Perform data cleaning and filtering\n",
    "            df = df[['Location Area Code(LAC)', 'Cell Identity(CI)', 'Start Time', 'End Time',\n",
    "                     'Duration', 'Uplink traffic', 'Downlink traffic', 'Radio Access Type(RAT)',\n",
    "                     'Source IP', 'Destination IP', 'URL', 'User Agent']]\n",
    "            \n",
    "            # Apply any additional preprocessing steps if required\n",
    "            \n",
    "            # Append the preprocessed data to the list\n",
    "            preprocessed_data.append(df)\n",
    "\n",
    "# Concatenate all the preprocessed dataframes into a single dataframe\n",
    "preprocessed_df = pd.concat(preprocessed_data, ignore_index=True)\n",
    "\n",
    "# Analyze content popularity\n",
    "content_popularity = preprocessed_df['URL'].value_counts()\n",
    "\n",
    "# Design caching policy using LRU algorithm\n",
    "cache_size = 1000  # Specify the cache size\n",
    "\n",
    "# Sort the content popularity in descending order\n",
    "sorted_content_popularity = content_popularity.sort_values(ascending=False)\n",
    "\n",
    "# Initialize the cache as an ordered dictionary with a limited size\n",
    "cache = OrderedDict()\n",
    "\n",
    "# Function to check if a content is in the cache\n",
    "def in_cache(content):\n",
    "    return content in cache\n",
    "\n",
    "# Function to access a content (update the LRU order)\n",
    "def access_content(content):\n",
    "    if in_cache(content):\n",
    "        # Move the accessed content to the end to update its recency\n",
    "        cache.move_to_end(content)\n",
    "\n",
    "# Function to add a content to the cache (evict the least recently used content if the cache is full)\n",
    "def add_content_to_cache(content):\n",
    "    if not in_cache(content):\n",
    "        if len(cache) >= cache_size:\n",
    "            # Remove the least recently used content\n",
    "            cache.popitem(last=False)\n",
    "        cache[content] = None\n",
    "        # Move the newly added content to the end to mark it as the most recently used\n",
    "\n",
    "# Define evaluation metrics\n",
    "total_requests = len(preprocessed_df)  # Total number of requests\n",
    "cache_hits = 0  # Number of cache hits\n",
    "traffic_offloaded = 0  # Amount of traffic offloaded to the cache\n",
    "\n",
    "# Iterate over the dataset\n",
    "for _, row in preprocessed_df.iterrows():\n",
    "    content = row['URL']\n",
    "    \n",
    "    # Check if the content is in the cache\n",
    "    if in_cache(content):\n",
    "        cache_hits += 1\n",
    "    else:\n",
    "        add_content_to_cache(content)\n",
    "        traffic_offloaded += row['Downlink traffic']\n",
    "        \n",
    "# Calculate hit ratio\n",
    "hit_ratio = cache_hits / total_requests\n",
    "\n",
    "# Calculate latency reduction (assuming each cache hit avoids a network round trip)\n",
    "latency_reduction = cache_hits / total_requests\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(\"Hit Ratio:\", hit_ratio)\n",
    "print(\"Latency Reduction:\", latency_reduction)\n",
    "print(\"Traffic Offloaded:\", traffic_offloaded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63253fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "LRU - Hit Ratio: 0.7624485531410047\n",
      "LRU - Latency Reduction: 0.7624485531410047\n",
      "LRU - Traffic Offloaded: 1629248642\n",
      "\n",
      "LFU - Hit Ratio: 0.7624485531410047\n",
      "LFU - Latency Reduction: 0.7624485531410047\n",
      "LFU - Traffic Offloaded: 1629248642\n",
      "\n",
      "Random - Hit Ratio: 0.7615062761506276\n",
      "Random - Latency Reduction: 0.7615062761506276\n",
      "Random - Traffic Offloaded: 1624149355\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "\n",
    "# Define the path to the dataset\n",
    "dataset_path = r'C:\\Users\\Administrator\\Downloads\\Edge-Computing-Dataset-master\\Edge-Computing-Dataset-master\\Data'\n",
    "\n",
    "# Define a list to store the preprocessed data\n",
    "preprocessed_data = []\n",
    "\n",
    "# Iterate over the subfolders\n",
    "for folder in ['shanghai', 'guangzhou']:\n",
    "    folder_path = os.path.join(dataset_path, folder)\n",
    "    \n",
    "    # Iterate over the CSV files in the subfolder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            \n",
    "            # Load the CSV file into a DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Perform data cleaning and filtering\n",
    "            df = df[['Location Area Code(LAC)', 'Cell Identity(CI)', 'Start Time', 'End Time',\n",
    "                     'Duration', 'Uplink traffic', 'Downlink traffic', 'Radio Access Type(RAT)',\n",
    "                     'Source IP', 'Destination IP', 'URL', 'User Agent']]\n",
    "            \n",
    "            # Apply any additional preprocessing steps if required\n",
    "            \n",
    "            # Append the preprocessed data to the list\n",
    "            preprocessed_data.append(df)\n",
    "\n",
    "# Concatenate all the preprocessed dataframes into a single dataframe\n",
    "preprocessed_df = pd.concat(preprocessed_data, ignore_index=True)\n",
    "\n",
    "# Analyze content popularity\n",
    "content_popularity = preprocessed_df['URL'].value_counts()\n",
    "\n",
    "# Design caching policy using LRU algorithm\n",
    "cache_size = 1000  # Specify the cache size\n",
    "\n",
    "# Sort the content popularity in descending order\n",
    "sorted_content_popularity = content_popularity.sort_values(ascending=False)\n",
    "\n",
    "# Define an ordered dictionary class that handles NoneType values properly\n",
    "class LFUCache(OrderedDict):\n",
    "    def __setitem__(self, key, value):\n",
    "        if key not in self:\n",
    "            OrderedDict.__setitem__(self, key, [value, 0])\n",
    "        self[key][1] += 1\n",
    "        self.move_to_end(key)\n",
    "\n",
    "# Initialize the caches with their respective cache replacement algorithms\n",
    "lru_cache = OrderedDict()\n",
    "lfu_cache = LFUCache()\n",
    "random_cache = OrderedDict()\n",
    "\n",
    "# Function to check if a content is in the cache\n",
    "def in_cache(cache, content):\n",
    "    return content in cache\n",
    "\n",
    "# Function to access a content (update the order for LRU and LFU, no need for Random)\n",
    "def access_content(cache, content):\n",
    "    if in_cache(cache, content):\n",
    "        if cache is not random_cache:\n",
    "            # Move the accessed content to the end to update its recency or frequency\n",
    "            cache.move_to_end(content)\n",
    "\n",
    "# Function to add a content to the cache (evict the content based on the cache replacement policy)\n",
    "def add_content_to_cache(cache, content, replacement_algo):\n",
    "    if not in_cache(cache, content):\n",
    "        if len(cache) >= cache_size:\n",
    "            if replacement_algo == 'LRU':\n",
    "                # Remove the least recently used content\n",
    "                cache.popitem(last=False)\n",
    "            elif replacement_algo == 'LFU':\n",
    "                # Find the least frequently used content\n",
    "                lfu_content = min(cache, key=lambda x: cache[x][1])\n",
    "                del cache[lfu_content]\n",
    "            elif replacement_algo == 'Random':\n",
    "                # Remove a random content\n",
    "                random_content = random.choice(list(cache.keys()))\n",
    "                del cache[random_content]\n",
    "        cache[content] = None\n",
    "        # Move the newly added content to the end to mark it as the most recently used/frequently used\n",
    "\n",
    "# Define evaluation metrics\n",
    "total_requests = len(preprocessed_df)  # Total number of requests\n",
    "cache_hits_lru = 0  # Number of cache hits for LRU\n",
    "cache_hits_lfu = 0  # Number of cache hits for LFU\n",
    "cache_hits_random = 0  # Number of cache hits for Random\n",
    "traffic_offloaded_lru = 0  # Amount of traffic offloaded to the cache for LRU\n",
    "traffic_offloaded_lfu = 0  # Amount of traffic offloaded to the cache for LFU\n",
    "traffic_offloaded_random = 0  # Amount of traffic offloaded to the cache for Random\n",
    "\n",
    "# Iterate over the dataset\n",
    "for _, row in preprocessed_df.iterrows():\n",
    "    content = row['URL']\n",
    "    \n",
    "    # Check if the content is in the LRU cache\n",
    "    if in_cache(lru_cache, content):\n",
    "        cache_hits_lru += 1\n",
    "    else:\n",
    "        add_content_to_cache(lru_cache, content, 'LRU')\n",
    "        traffic_offloaded_lru += row['Downlink traffic']\n",
    "    \n",
    "    # Check if the content is in the LFU cache\n",
    "    if in_cache(lfu_cache, content):\n",
    "        cache_hits_lfu += 1\n",
    "    else:\n",
    "        add_content_to_cache(lfu_cache, content, 'LFU')\n",
    "        traffic_offloaded_lfu += row['Downlink traffic']\n",
    "    \n",
    "    # Check if the content is in the Random cache\n",
    "    if in_cache(random_cache, content):\n",
    "        cache_hits_random += 1\n",
    "    else:\n",
    "        add_content_to_cache(random_cache, content, 'Random')\n",
    "        traffic_offloaded_random += row['Downlink traffic']\n",
    "        \n",
    "# Calculate hit ratios\n",
    "hit_ratio_lru = cache_hits_lru / total_requests\n",
    "hit_ratio_lfu = cache_hits_lfu / total_requests\n",
    "hit_ratio_random = cache_hits_random / total_requests\n",
    "\n",
    "# Calculate latency reductions (assuming each cache hit avoids a network round trip)\n",
    "latency_reduction_lru = cache_hits_lru / total_requests\n",
    "latency_reduction_lfu = cache_hits_lfu / total_requests\n",
    "latency_reduction_random = cache_hits_random / total_requests\n",
    "\n",
    "# Print the evaluation metrics for each cache replacement algorithm\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(\"LRU - Hit Ratio:\", hit_ratio_lru)\n",
    "print(\"LRU - Latency Reduction:\", latency_reduction_lru)\n",
    "print(\"LRU - Traffic Offloaded:\", traffic_offloaded_lru)\n",
    "print()\n",
    "print(\"LFU - Hit Ratio:\", hit_ratio_lfu)\n",
    "print(\"LFU - Latency Reduction:\", latency_reduction_lfu)\n",
    "print(\"LFU - Traffic Offloaded:\", traffic_offloaded_lfu)\n",
    "print()\n",
    "print(\"Random - Hit Ratio:\", hit_ratio_random)\n",
    "print(\"Random - Latency Reduction:\", latency_reduction_random)\n",
    "print(\"Random - Traffic Offloaded:\", traffic_offloaded_random)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cf27b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "LRU - Hit Ratio: 0.7624485531410047\n",
      "LRU - Latency Reduction: 0.7624485531410047\n",
      "LRU - Traffic Offloaded: 1629248642\n",
      "\n",
      "CAR - Hit Ratio: 0.7624485531410047\n",
      "CAR - Latency Reduction: 0.7624485531410047\n",
      "CAR - Traffic Offloaded: 1629248642\n",
      "\n",
      "Random - Hit Ratio: 0.7620530897408982\n",
      "Random - Latency Reduction: 0.7620530897408982\n",
      "Random - Traffic Offloaded: 1644536389\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "\n",
    "# Define the path to the dataset\n",
    "dataset_path = r'C:\\Users\\Administrator\\Downloads\\Edge-Computing-Dataset-master\\Edge-Computing-Dataset-master\\Data'\n",
    "\n",
    "# Define a list to store the preprocessed data\n",
    "preprocessed_data = []\n",
    "\n",
    "# Iterate over the subfolders\n",
    "for folder in ['shanghai', 'guangzhou']:\n",
    "    folder_path = os.path.join(dataset_path, folder)\n",
    "    \n",
    "    # Iterate over the CSV files in the subfolder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            \n",
    "            # Load the CSV file into a DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Perform data cleaning and filtering\n",
    "            df = df[['Location Area Code(LAC)', 'Cell Identity(CI)', 'Start Time', 'End Time',\n",
    "                     'Duration', 'Uplink traffic', 'Downlink traffic', 'Radio Access Type(RAT)',\n",
    "                     'Source IP', 'Destination IP', 'URL', 'User Agent']]\n",
    "            \n",
    "            # Apply any additional preprocessing steps if required\n",
    "            \n",
    "            # Append the preprocessed data to the list\n",
    "            preprocessed_data.append(df)\n",
    "\n",
    "# Concatenate all the preprocessed dataframes into a single dataframe\n",
    "preprocessed_df = pd.concat(preprocessed_data, ignore_index=True)\n",
    "\n",
    "# Analyze content popularity\n",
    "content_popularity = preprocessed_df['URL'].value_counts()\n",
    "\n",
    "# Design caching policy using LRU algorithm\n",
    "cache_size = 1000  # Specify the cache size\n",
    "\n",
    "# Sort the content popularity in descending order\n",
    "sorted_content_popularity = content_popularity.sort_values(ascending=False)\n",
    "\n",
    "# Define an ordered dictionary class that handles NoneType values properly\n",
    "class CostAwareLRUCache(OrderedDict):\n",
    "    def __init__(self, cache_size):\n",
    "        self.cache_size = cache_size\n",
    "        self.total_cost = 0\n",
    "        super().__init__()\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        if key not in self:\n",
    "            OrderedDict.__setitem__(self, key, [value, 0])\n",
    "        self[key][1] += 1\n",
    "        self.move_to_end(key)\n",
    "        self.total_cost += 1\n",
    "\n",
    "    def evict(self):\n",
    "        if len(self) >= self.cache_size:\n",
    "            min_cost_content = min(self, key=lambda x: self.get_content_cost(x))\n",
    "            del self[min_cost_content]\n",
    "            self.total_cost -= 1\n",
    "\n",
    "    def get_content_cost(self, content):\n",
    "        return self[content][1] / self.total_cost if self.total_cost > 0 else 0\n",
    "\n",
    "    def get_cache_cost(self):\n",
    "        return len(self) / self.cache_size if self.cache_size > 0 else 0\n",
    "\n",
    "# Initialize the caches with their respective cache replacement algorithms\n",
    "lru_cache = OrderedDict()\n",
    "car_cache = CostAwareLRUCache(cache_size)\n",
    "random_cache = OrderedDict()\n",
    "\n",
    "# Function to check if a content is in the cache\n",
    "def in_cache(cache, content):\n",
    "    return content in cache\n",
    "\n",
    "# Function to access a content (update the order for LRU and CAR, no need for Random)\n",
    "def access_content(cache, content):\n",
    "    if in_cache(cache, content):\n",
    "        if cache is not random_cache:\n",
    "            # Move the accessed content to the end to update its recency or cost\n",
    "            cache.move_to_end(content)\n",
    "\n",
    "# Function to add a content to the cache (evict the content based on the cache replacement policy)\n",
    "def add_content_to_cache(cache, content, replacement_algo):\n",
    "    if not in_cache(cache, content):\n",
    "        if len(cache) >= cache_size:\n",
    "            if replacement_algo == 'LRU':\n",
    "                # Remove the least recently used content\n",
    "                cache.popitem(last=False)\n",
    "            elif replacement_algo == 'CAR':\n",
    "                # Evict content based on the CAR replacement policy\n",
    "                cache.evict()\n",
    "            elif replacement_algo == 'Random':\n",
    "                # Remove a random content\n",
    "                random_content = random.choice(list(cache.keys()))\n",
    "                del cache[random_content]\n",
    "        cache[content] = None\n",
    "        # Move the newly added content to the end to mark it as the most recently used/costly\n",
    "\n",
    "# Define evaluation metrics\n",
    "total_requests = len(preprocessed_df)  # Total number of requests\n",
    "cache_hits_lru = 0  # Number of cache hits for LRU\n",
    "cache_hits_car = 0  # Number of cache hits for CAR\n",
    "cache_hits_random = 0  # Number of cache hits for Random\n",
    "traffic_offloaded_lru = 0  # Amount of traffic offloaded to the cache for LRU\n",
    "traffic_offloaded_car = 0  # Amount of traffic offloaded to the cache for CAR\n",
    "traffic_offloaded_random = 0  # Amount of traffic offloaded to the cache for Random\n",
    "\n",
    "# Iterate over the dataset\n",
    "for _, row in preprocessed_df.iterrows():\n",
    "    content = row['URL']\n",
    "    \n",
    "    # Check if the content is in the LRU cache\n",
    "    if in_cache(lru_cache, content):\n",
    "        cache_hits_lru += 1\n",
    "    else:\n",
    "        add_content_to_cache(lru_cache, content, 'LRU')\n",
    "        traffic_offloaded_lru += row['Downlink traffic']\n",
    "    \n",
    "    # Check if the content is in the CAR cache\n",
    "    if in_cache(car_cache, content):\n",
    "        cache_hits_car += 1\n",
    "    else:\n",
    "        add_content_to_cache(car_cache, content, 'CAR')\n",
    "        traffic_offloaded_car += row['Downlink traffic']\n",
    "    \n",
    "    # Check if the content is in the Random cache\n",
    "    if in_cache(random_cache, content):\n",
    "        cache_hits_random += 1\n",
    "    else:\n",
    "        add_content_to_cache(random_cache, content, 'Random')\n",
    "        traffic_offloaded_random += row['Downlink traffic']\n",
    "        \n",
    "# Calculate hit ratios\n",
    "hit_ratio_lru = cache_hits_lru / total_requests\n",
    "hit_ratio_car = cache_hits_car / total_requests\n",
    "hit_ratio_random = cache_hits_random / total_requests\n",
    "\n",
    "# Calculate latency reductions (assuming each cache hit avoids a network round trip)\n",
    "latency_reduction_lru = cache_hits_lru / total_requests\n",
    "latency_reduction_car = cache_hits_car / total_requests\n",
    "latency_reduction_random = cache_hits_random / total_requests\n",
    "\n",
    "# Print the evaluation metrics for each cache replacement algorithm\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(\"LRU - Hit Ratio:\", hit_ratio_lru)\n",
    "print(\"LRU - Latency Reduction:\", latency_reduction_lru)\n",
    "print(\"LRU - Traffic Offloaded:\", traffic_offloaded_lru)\n",
    "print()\n",
    "print(\"CAR - Hit Ratio:\", hit_ratio_car)\n",
    "print(\"CAR - Latency Reduction:\", latency_reduction_car)\n",
    "print(\"CAR - Traffic Offloaded:\", traffic_offloaded_car)\n",
    "print()\n",
    "print(\"Random - Hit Ratio:\", hit_ratio_random)\n",
    "print(\"Random - Latency Reduction:\", latency_reduction_random)\n",
    "print(\"Random - Traffic Offloaded:\", traffic_offloaded_random)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a243820e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "LRU - Hit Ratio: 0.7624485531410047\n",
      "LRU - Latency Reduction: 0.7624485531410047\n",
      "LRU - Traffic Offloaded: 1629248642\n",
      "LRU - Network Latency Reduction: 38.12242765705023\n",
      "LRU - Network Bandwidth Saved: -795342.2315853201\n",
      "LRU - Network Cost Reduction: 1629248.642\n",
      "\n",
      "CAR - Hit Ratio: 0.7624485531410047\n",
      "CAR - Latency Reduction: 0.7624485531410047\n",
      "CAR - Traffic Offloaded: 1629248642\n",
      "CAR - Network Latency Reduction: 38.12242765705023\n",
      "CAR - Network Bandwidth Saved: -795342.2315853201\n",
      "CAR - Network Cost Reduction: 1629248.642\n",
      "\n",
      "Random - Hit Ratio: 0.7613305146394692\n",
      "Random - Latency Reduction: 0.7613305146394692\n",
      "Random - Traffic Offloaded: 1630607824\n",
      "Random - Network Latency Reduction: 38.066525731973464\n",
      "Random - Network Bandwidth Saved: -796005.8201471515\n",
      "Random - Network Cost Reduction: 1630607.824\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "\n",
    "# Define the path to the dataset\n",
    "dataset_path = r'C:\\Users\\Administrator\\Downloads\\Edge-Computing-Dataset-master\\Edge-Computing-Dataset-master\\Data'\n",
    "\n",
    "# Define a list to store the preprocessed data\n",
    "preprocessed_data = []\n",
    "\n",
    "# Iterate over the subfolders\n",
    "for folder in ['shanghai', 'guangzhou']:\n",
    "    folder_path = os.path.join(dataset_path, folder)\n",
    "    \n",
    "    # Iterate over the CSV files in the subfolder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            \n",
    "            # Load the CSV file into a DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Perform data cleaning and filtering\n",
    "            df = df[['Location Area Code(LAC)', 'Cell Identity(CI)', 'Start Time', 'End Time',\n",
    "                     'Duration', 'Uplink traffic', 'Downlink traffic', 'Radio Access Type(RAT)',\n",
    "                     'Source IP', 'Destination IP', 'URL', 'User Agent']]\n",
    "            \n",
    "            # Apply any additional preprocessing steps if required\n",
    "            \n",
    "            # Append the preprocessed data to the list\n",
    "            preprocessed_data.append(df)\n",
    "\n",
    "# Concatenate all the preprocessed dataframes into a single dataframe\n",
    "preprocessed_df = pd.concat(preprocessed_data, ignore_index=True)\n",
    "\n",
    "# Analyze content popularity\n",
    "content_popularity = preprocessed_df['URL'].value_counts()\n",
    "\n",
    "# Design caching policy using LRU algorithm\n",
    "cache_size = 1000  # Specify the cache size\n",
    "\n",
    "# Sort the content popularity in descending order\n",
    "sorted_content_popularity = content_popularity.sort_values(ascending=False)\n",
    "\n",
    "# Define an ordered dictionary class that handles NoneType values properly\n",
    "class CostAwareLRUCache(OrderedDict):\n",
    "    def __init__(self, cache_size):\n",
    "        self.cache_size = cache_size\n",
    "        self.total_cost = 0\n",
    "        super().__init__()\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        if key not in self:\n",
    "            OrderedDict.__setitem__(self, key, [value, 0])\n",
    "        self[key][1] += 1\n",
    "        self.move_to_end(key)\n",
    "        self.total_cost += 1\n",
    "\n",
    "    def evict(self):\n",
    "        if len(self) >= self.cache_size:\n",
    "            min_cost_content = min(self, key=lambda x: self.get_content_cost(x))\n",
    "            del self[min_cost_content]\n",
    "            self.total_cost -= 1\n",
    "\n",
    "    def get_content_cost(self, content):\n",
    "        return self[content][1] / self.total_cost if self.total_cost > 0 else 0\n",
    "\n",
    "    def get_cache_cost(self):\n",
    "        return len(self) / self.cache_size if self.cache_size > 0 else 0\n",
    "\n",
    "# Initialize the caches with their respective cache replacement algorithms\n",
    "lru_cache = OrderedDict()\n",
    "car_cache = CostAwareLRUCache(cache_size)\n",
    "random_cache = OrderedDict()\n",
    "\n",
    "# Function to check if a content is in the cache\n",
    "def in_cache(cache, content):\n",
    "    return content in cache\n",
    "\n",
    "# Function to access a content (update the order for LRU and CAR, no need for Random)\n",
    "def access_content(cache, content):\n",
    "    if in_cache(cache, content):\n",
    "        if cache is not random_cache:\n",
    "            # Move the accessed content to the end to update its recency or cost\n",
    "            cache.move_to_end(content)\n",
    "\n",
    "# Function to add a content to the cache (evict the content based on the cache replacement policy)\n",
    "def add_content_to_cache(cache, content, replacement_algo):\n",
    "    if not in_cache(cache, content):\n",
    "        if len(cache) >= cache_size:\n",
    "            if replacement_algo == 'LRU':\n",
    "                # Remove the least recently used content\n",
    "                cache.popitem(last=False)\n",
    "            elif replacement_algo == 'CAR':\n",
    "                # Evict content based on the CAR replacement policy\n",
    "                cache.evict()\n",
    "            elif replacement_algo == 'Random':\n",
    "                # Remove a random content\n",
    "                random_content = random.choice(list(cache.keys()))\n",
    "                del cache[random_content]\n",
    "        cache[content] = None\n",
    "        # Move the newly added content to the end to mark it as the most recently used/costly\n",
    "\n",
    "# Define evaluation metrics\n",
    "total_requests = len(preprocessed_df)  # Total number of requests\n",
    "cache_hits_lru = 0  # Number of cache hits for LRU\n",
    "cache_hits_car = 0  # Number of cache hits for CAR\n",
    "cache_hits_random = 0  # Number of cache hits for Random\n",
    "traffic_offloaded_lru = 0  # Amount of traffic offloaded to the cache for LRU\n",
    "traffic_offloaded_car = 0  # Amount of traffic offloaded to the cache for CAR\n",
    "traffic_offloaded_random = 0  # Amount of traffic offloaded to the cache for Random\n",
    "\n",
    "# Additional evaluation metrics\n",
    "network_latency = 50  # Example network latency in milliseconds\n",
    "network_bandwidth = 100  # Example network bandwidth in Mbps\n",
    "network_cost_per_byte = 0.001  # Example cost per byte in currency units\n",
    "\n",
    "# Iterate over the dataset\n",
    "for _, row in preprocessed_df.iterrows():\n",
    "    content = row['URL']\n",
    "    \n",
    "    # Check if the content is in the LRU cache\n",
    "    if in_cache(lru_cache, content):\n",
    "        cache_hits_lru += 1\n",
    "    else:\n",
    "        add_content_to_cache(lru_cache, content, 'LRU')\n",
    "        traffic_offloaded_lru += row['Downlink traffic']\n",
    "    \n",
    "    # Check if the content is in the CAR cache\n",
    "    if in_cache(car_cache, content):\n",
    "        cache_hits_car += 1\n",
    "    else:\n",
    "        add_content_to_cache(car_cache, content, 'CAR')\n",
    "        traffic_offloaded_car += row['Downlink traffic']\n",
    "    \n",
    "    # Check if the content is in the Random cache\n",
    "    if in_cache(random_cache, content):\n",
    "        cache_hits_random += 1\n",
    "    else:\n",
    "        add_content_to_cache(random_cache, content, 'Random')\n",
    "        traffic_offloaded_random += row['Downlink traffic']\n",
    "        \n",
    "# Calculate hit ratios\n",
    "hit_ratio_lru = cache_hits_lru / total_requests\n",
    "hit_ratio_car = cache_hits_car / total_requests\n",
    "hit_ratio_random = cache_hits_random / total_requests\n",
    "\n",
    "# Calculate latency reductions (assuming each cache hit avoids a network round trip)\n",
    "latency_reduction_lru = cache_hits_lru / total_requests\n",
    "latency_reduction_car = cache_hits_car / total_requests\n",
    "latency_reduction_random = cache_hits_random / total_requests\n",
    "\n",
    "# Calculate additional evaluation metrics\n",
    "network_latency_reduction_lru = latency_reduction_lru * network_latency\n",
    "network_latency_reduction_car = latency_reduction_car * network_latency\n",
    "network_latency_reduction_random = latency_reduction_random * network_latency\n",
    "\n",
    "network_bandwidth_saved_lru = (1 - traffic_offloaded_lru / total_requests) * network_bandwidth\n",
    "network_bandwidth_saved_car = (1 - traffic_offloaded_car / total_requests) * network_bandwidth\n",
    "network_bandwidth_saved_random = (1 - traffic_offloaded_random / total_requests) * network_bandwidth\n",
    "\n",
    "network_cost_reduction_lru = traffic_offloaded_lru * network_cost_per_byte\n",
    "network_cost_reduction_car = traffic_offloaded_car * network_cost_per_byte\n",
    "network_cost_reduction_random = traffic_offloaded_random * network_cost_per_byte\n",
    "\n",
    "# Print the evaluation metrics for each cache replacement algorithm\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(\"LRU - Hit Ratio:\", hit_ratio_lru)\n",
    "print(\"LRU - Latency Reduction:\", latency_reduction_lru)\n",
    "print(\"LRU - Traffic Offloaded:\", traffic_offloaded_lru)\n",
    "print(\"LRU - Network Latency Reduction:\", network_latency_reduction_lru)\n",
    "print(\"LRU - Network Bandwidth Saved:\", network_bandwidth_saved_lru)\n",
    "print(\"LRU - Network Cost Reduction:\", network_cost_reduction_lru)\n",
    "print()\n",
    "print(\"CAR - Hit Ratio:\", hit_ratio_car)\n",
    "print(\"CAR - Latency Reduction:\", latency_reduction_car)\n",
    "print(\"CAR - Traffic Offloaded:\", traffic_offloaded_car)\n",
    "print(\"CAR - Network Latency Reduction:\", network_latency_reduction_car)\n",
    "print(\"CAR - Network Bandwidth Saved:\", network_bandwidth_saved_car)\n",
    "print(\"CAR - Network Cost Reduction:\", network_cost_reduction_car)\n",
    "print()\n",
    "print(\"Random - Hit Ratio:\", hit_ratio_random)\n",
    "print(\"Random - Latency Reduction:\", latency_reduction_random)\n",
    "print(\"Random - Traffic Offloaded:\", traffic_offloaded_random)\n",
    "print(\"Random - Network Latency Reduction:\", network_latency_reduction_random)\n",
    "print(\"Random - Network Bandwidth Saved:\", network_bandwidth_saved_random)\n",
    "print(\"Random - Network Cost Reduction:\", network_cost_reduction_random)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03d08d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "LRU - Hit Ratio: 0.7624485531410047\n",
      "LRU - Latency Reduction: 0.7624485531410047\n",
      "LRU - Traffic Offloaded: 1629248642\n",
      "LRU - Network Latency Reduction: 38.12242765705023\n",
      "LRU - Network Bandwidth Saved: -795342.2315853201\n",
      "LRU - Network Cost Reduction: 1629248.642\n",
      "\n",
      "CAR - Hit Ratio: 0.7624485531410047\n",
      "CAR - Latency Reduction: 0.7624485531410047\n",
      "CAR - Traffic Offloaded: 1629248642\n",
      "CAR - Network Latency Reduction: 38.12242765705023\n",
      "CAR - Network Bandwidth Saved: -795342.2315853201\n",
      "CAR - Network Cost Reduction: 1629248.642\n",
      "\n",
      "Random - Hit Ratio: 0.7617650361531664\n",
      "Random - Latency Reduction: 0.7617650361531664\n",
      "Random - Traffic Offloaded: 1631544852\n",
      "Random - Network Latency Reduction: 38.08825180765832\n",
      "Random - Network Bandwidth Saved: -796463.301972923\n",
      "Random - Network Cost Reduction: 1631544.852\n",
      "\n",
      "Network-Aware - Hit Ratio: 0.004882264198844856\n",
      "Network-Aware - Latency Reduction: 0.004882264198844856\n",
      "Network-Aware - Traffic Offloaded: 1629248642\n",
      "Network-Aware - Network Latency Reduction: 0.22946641734570825\n",
      "Network-Aware - Network Bandwidth Saved: -819202.4985328796\n",
      "Network-Aware - Network Cost Reduction: 16292486.42\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "\n",
    "# Define the path to the dataset\n",
    "dataset_path = r'C:\\Users\\Administrator\\Downloads\\Edge-Computing-Dataset-master\\Edge-Computing-Dataset-master\\Data'\n",
    "\n",
    "# Define a list to store the preprocessed data\n",
    "preprocessed_data = []\n",
    "\n",
    "# Iterate over the subfolders\n",
    "for folder in ['shanghai', 'guangzhou']:\n",
    "    folder_path = os.path.join(dataset_path, folder)\n",
    "    \n",
    "    # Iterate over the CSV files in the subfolder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            \n",
    "            # Load the CSV file into a DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Perform data cleaning and filtering\n",
    "            df = df[['Location Area Code(LAC)', 'Cell Identity(CI)', 'Start Time', 'End Time',\n",
    "                     'Duration', 'Uplink traffic', 'Downlink traffic', 'Radio Access Type(RAT)',\n",
    "                     'Source IP', 'Destination IP', 'URL', 'User Agent']]\n",
    "            \n",
    "            # Apply any additional preprocessing steps if required\n",
    "            \n",
    "            # Append the preprocessed data to the list\n",
    "            preprocessed_data.append(df)\n",
    "\n",
    "# Concatenate all the preprocessed dataframes into a single dataframe\n",
    "preprocessed_df = pd.concat(preprocessed_data, ignore_index=True)\n",
    "\n",
    "# Analyze content popularity\n",
    "content_popularity = preprocessed_df['URL'].value_counts()\n",
    "\n",
    "# Design caching policy using LRU algorithm\n",
    "cache_size = 1000  # Specify the cache size\n",
    "\n",
    "# Sort the content popularity in descending order\n",
    "sorted_content_popularity = content_popularity.sort_values(ascending=False)\n",
    "\n",
    "# Define an ordered dictionary class that handles NoneType values properly\n",
    "class CostAwareLRUCache(OrderedDict):\n",
    "    def __init__(self, cache_size):\n",
    "        self.cache_size = cache_size\n",
    "        self.total_cost = 0\n",
    "        super().__init__()\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        if key not in self:\n",
    "            OrderedDict.__setitem__(self, key, [value, 0])\n",
    "        self[key][1] += 1\n",
    "        self.move_to_end(key)\n",
    "        self.total_cost += 1\n",
    "\n",
    "    def evict(self):\n",
    "        if len(self) >= self.cache_size:\n",
    "            min_cost_content = min(self, key=lambda x: self.get_content_cost(x))\n",
    "            del self[min_cost_content]\n",
    "            self.total_cost -= 1\n",
    "\n",
    "    def get_content_cost(self, content):\n",
    "        return self[content][1] / self.total_cost if self.total_cost > 0 else 0\n",
    "\n",
    "    def get_cache_cost(self):\n",
    "        return len(self) / self.cache_size if self.cache_size > 0 else 0\n",
    "\n",
    "# Initialize the caches with their respective cache replacement algorithms\n",
    "lru_cache = OrderedDict()\n",
    "car_cache = CostAwareLRUCache(cache_size)\n",
    "random_cache = OrderedDict()\n",
    "\n",
    "# Function to check if a content is in the cache\n",
    "def in_cache(cache, content):\n",
    "    return content in cache\n",
    "\n",
    "# Function to access a content (update the order for LRU and CAR, no need for Random)\n",
    "def access_content(cache, content):\n",
    "    if in_cache(cache, content):\n",
    "        if cache is not random_cache:\n",
    "            # Move the accessed content to the end to update its recency or cost\n",
    "            cache.move_to_end(content)\n",
    "\n",
    "# Function to add a content to the cache (evict the content based on the cache replacement policy)\n",
    "def add_content_to_cache(cache, content, replacement_algo):\n",
    "    if not in_cache(cache, content):\n",
    "        if len(cache) >= cache_size:\n",
    "            if replacement_algo == 'LRU':\n",
    "                # Remove the least recently used content\n",
    "                cache.popitem(last=False)\n",
    "            elif replacement_algo == 'CAR':\n",
    "                # Evict content based on the CAR replacement policy\n",
    "                cache.evict()\n",
    "            elif replacement_algo == 'Random':\n",
    "                # Remove a random content\n",
    "                random_content = random.choice(list(cache.keys()))\n",
    "                del cache[random_content]\n",
    "        cache[content] = None\n",
    "        # Move the newly added content to the end to mark it as the most recently used/costly\n",
    "\n",
    "# Define evaluation metrics\n",
    "total_requests = len(preprocessed_df)  # Total number of requests\n",
    "cache_hits_lru = 0  # Number of cache hits for LRU\n",
    "cache_hits_car = 0  # Number of cache hits for CAR\n",
    "cache_hits_random = 0  # Number of cache hits for Random\n",
    "traffic_offloaded_lru = 0  # Amount of traffic offloaded to the cache for LRU\n",
    "traffic_offloaded_car = 0  # Amount of traffic offloaded to the cache for CAR\n",
    "traffic_offloaded_random = 0  # Amount of traffic offloaded to the cache for Random\n",
    "\n",
    "# Additional evaluation metrics\n",
    "network_latency = 50  # Example network latency in milliseconds\n",
    "network_bandwidth = 100  # Example network bandwidth in Mbps\n",
    "network_cost_per_byte = 0.001  # Example cost per byte in currency units\n",
    "\n",
    "# Iterate over the dataset\n",
    "for _, row in preprocessed_df.iterrows():\n",
    "    content = row['URL']\n",
    "    \n",
    "    # Check if the content is in the LRU cache\n",
    "    if in_cache(lru_cache, content):\n",
    "        cache_hits_lru += 1\n",
    "    else:\n",
    "        add_content_to_cache(lru_cache, content, 'LRU')\n",
    "        traffic_offloaded_lru += row['Downlink traffic']\n",
    "    \n",
    "    # Check if the content is in the CAR cache\n",
    "    if in_cache(car_cache, content):\n",
    "        cache_hits_car += 1\n",
    "    else:\n",
    "        add_content_to_cache(car_cache, content, 'CAR')\n",
    "        traffic_offloaded_car += row['Downlink traffic']\n",
    "    \n",
    "    # Check if the content is in the Random cache\n",
    "    if in_cache(random_cache, content):\n",
    "        cache_hits_random += 1\n",
    "    else:\n",
    "        add_content_to_cache(random_cache, content, 'Random')\n",
    "        traffic_offloaded_random += row['Downlink traffic']\n",
    "        \n",
    "# Calculate hit ratios\n",
    "hit_ratio_lru = cache_hits_lru / total_requests\n",
    "hit_ratio_car = cache_hits_car / total_requests\n",
    "hit_ratio_random = cache_hits_random / total_requests\n",
    "\n",
    "# Calculate latency reductions (assuming each cache hit avoids a network round trip)\n",
    "latency_reduction_lru = cache_hits_lru / total_requests\n",
    "latency_reduction_car = cache_hits_car / total_requests\n",
    "latency_reduction_random = cache_hits_random / total_requests\n",
    "\n",
    "# Calculate additional evaluation metrics\n",
    "network_latency_reduction_lru = latency_reduction_lru * network_latency\n",
    "network_latency_reduction_car = latency_reduction_car * network_latency\n",
    "network_latency_reduction_random = latency_reduction_random * network_latency\n",
    "\n",
    "network_bandwidth_saved_lru = (1 - traffic_offloaded_lru / total_requests) * network_bandwidth\n",
    "network_bandwidth_saved_car = (1 - traffic_offloaded_car / total_requests) * network_bandwidth\n",
    "network_bandwidth_saved_random = (1 - traffic_offloaded_random / total_requests) * network_bandwidth\n",
    "\n",
    "network_cost_reduction_lru = traffic_offloaded_lru * network_cost_per_byte\n",
    "network_cost_reduction_car = traffic_offloaded_car * network_cost_per_byte\n",
    "network_cost_reduction_random = traffic_offloaded_random * network_cost_per_byte\n",
    "\n",
    "# Additional cache admission policy based on network parameters\n",
    "class NetworkAwareCache(CostAwareLRUCache):\n",
    "    def __init__(self, cache_size, network_latency_threshold, network_bandwidth_threshold):\n",
    "        self.network_latency_threshold = network_latency_threshold\n",
    "        self.network_bandwidth_threshold = network_bandwidth_threshold\n",
    "        super().__init__(cache_size)\n",
    "\n",
    "    def should_cache_content(self, content, network_latency, network_bandwidth):\n",
    "        if network_latency < self.network_latency_threshold and network_bandwidth > self.network_bandwidth_threshold:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "# Adjust the network thresholds and cost assumptions\n",
    "network_latency_threshold = 100  # Adjust the network latency threshold\n",
    "network_bandwidth_threshold = 500  # Adjust the network bandwidth threshold\n",
    "network_cost_per_byte = 0.01  # Adjust the cost per byte assumption\n",
    "\n",
    "# Initialize the network-aware cache with its cache admission policy and network thresholds\n",
    "network_aware_cache = NetworkAwareCache(cache_size, network_latency_threshold, network_bandwidth_threshold)\n",
    "\n",
    "# Iterate over the dataset with network parameters\n",
    "for _, row in preprocessed_df.iterrows():\n",
    "    content = row['URL']\n",
    "    network_latency = random.randint(20, 60)  # Example network latency for the content request\n",
    "    network_bandwidth = random.randint(80, 120)  # Example network bandwidth for the content request\n",
    "    \n",
    "    if network_aware_cache.should_cache_content(content, network_latency, network_bandwidth):\n",
    "        access_content(network_aware_cache, content)\n",
    "    else:\n",
    "        add_content_to_cache(network_aware_cache, content, 'CAR')\n",
    "\n",
    "# Calculate hit ratio, latency reduction, and traffic offloaded for the network-aware cache\n",
    "cache_hits_network_aware = len(network_aware_cache)  # Assuming the cache replacement policy is the same as CAR\n",
    "hit_ratio_network_aware = cache_hits_network_aware / total_requests\n",
    "latency_reduction_network_aware = cache_hits_network_aware / total_requests\n",
    "traffic_offloaded_network_aware = traffic_offloaded_car  # Assuming the cache replacement policy is the same as CAR\n",
    "\n",
    "# Calculate additional evaluation metrics for the network-aware cache\n",
    "network_latency_reduction_network_aware = latency_reduction_network_aware * network_latency\n",
    "network_bandwidth_saved_network_aware = (1 - traffic_offloaded_network_aware / total_requests) * network_bandwidth\n",
    "network_cost_reduction_network_aware = traffic_offloaded_network_aware * network_cost_per_byte\n",
    "\n",
    "# Print the evaluation metrics for each cache replacement algorithm\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(\"LRU - Hit Ratio:\", hit_ratio_lru)\n",
    "print(\"LRU - Latency Reduction:\", latency_reduction_lru)\n",
    "print(\"LRU - Traffic Offloaded:\", traffic_offloaded_lru)\n",
    "print(\"LRU - Network Latency Reduction:\", network_latency_reduction_lru)\n",
    "print(\"LRU - Network Bandwidth Saved:\", network_bandwidth_saved_lru)\n",
    "print(\"LRU - Network Cost Reduction:\", network_cost_reduction_lru)\n",
    "print()\n",
    "print(\"CAR - Hit Ratio:\", hit_ratio_car)\n",
    "print(\"CAR - Latency Reduction:\", latency_reduction_car)\n",
    "print(\"CAR - Traffic Offloaded:\", traffic_offloaded_car)\n",
    "print(\"CAR - Network Latency Reduction:\", network_latency_reduction_car)\n",
    "print(\"CAR - Network Bandwidth Saved:\", network_bandwidth_saved_car)\n",
    "print(\"CAR - Network Cost Reduction:\", network_cost_reduction_car)\n",
    "print()\n",
    "print(\"Random - Hit Ratio:\", hit_ratio_random)\n",
    "print(\"Random - Latency Reduction:\", latency_reduction_random)\n",
    "print(\"Random - Traffic Offloaded:\", traffic_offloaded_random)\n",
    "print(\"Random - Network Latency Reduction:\", network_latency_reduction_random)\n",
    "print(\"Random - Network Bandwidth Saved:\", network_bandwidth_saved_random)\n",
    "print(\"Random - Network Cost Reduction:\", network_cost_reduction_random)\n",
    "print()\n",
    "print(\"Network-Aware - Hit Ratio:\", hit_ratio_network_aware)\n",
    "print(\"Network-Aware - Latency Reduction:\", latency_reduction_network_aware)\n",
    "print(\"Network-Aware - Traffic Offloaded:\", traffic_offloaded_network_aware)\n",
    "print(\"Network-Aware - Network Latency Reduction:\", network_latency_reduction_network_aware)\n",
    "print(\"Network-Aware - Network Bandwidth Saved:\", network_bandwidth_saved_network_aware)\n",
    "print(\"Network-Aware - Network Cost Reduction:\", network_cost_reduction_network_aware)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afc2ed48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "LRU - Hit Ratio: 0.7624485531410047\n",
      "LRU - Latency Reduction: 0.7624485531410047\n",
      "LRU - Traffic Offloaded: 1629248642\n",
      "LRU - Network Latency Reduction: 38.12242765705023\n",
      "LRU - Network Bandwidth Saved: -795342.2315853201\n",
      "LRU - Network Cost Reduction: 1629248.642\n",
      "\n",
      "CAR - Hit Ratio: 0.7624485531410047\n",
      "CAR - Latency Reduction: 0.7624485531410047\n",
      "CAR - Traffic Offloaded: 1629248642\n",
      "CAR - Network Latency Reduction: 38.12242765705023\n",
      "CAR - Network Bandwidth Saved: -795342.2315853201\n",
      "CAR - Network Cost Reduction: 1629248.642\n",
      "\n",
      "Random - Hit Ratio: 0.7618138587951548\n",
      "Random - Latency Reduction: 0.7618138587951548\n",
      "Random - Traffic Offloaded: 1631713206\n",
      "Random - Network Latency Reduction: 38.090692939757744\n",
      "Random - Network Bandwidth Saved: -796545.4968436161\n",
      "Random - Network Cost Reduction: 1631713.206\n",
      "\n",
      "Network-Aware - Hit Ratio: 0.004882264198844856\n",
      "Network-Aware - Latency Reduction: 0.004882264198844856\n",
      "Network-Aware - Traffic Offloaded: 1629248642\n",
      "Network-Aware - Network Latency Reduction: 0.2587600025387774\n",
      "Network-Aware - Network Bandwidth Saved: -755575.120006054\n",
      "Network-Aware - Network Cost Reduction: 16292486.42\n",
      "\n",
      "Dynamic - Hit Ratio: 0.0\n",
      "Dynamic - Latency Reduction: 0.0\n",
      "Dynamic - Traffic Offloaded: 1629248642\n",
      "Dynamic - Network Latency Reduction: 0.0\n",
      "Dynamic - Network Bandwidth Saved: -819202.4985328796\n",
      "Dynamic - Network Cost Reduction: 16292486.42\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "\n",
    "# Define the path to the dataset\n",
    "dataset_path = r'C:\\Users\\Administrator\\Downloads\\Edge-Computing-Dataset-master\\Edge-Computing-Dataset-master\\Data'\n",
    "\n",
    "# Define a list to store the preprocessed data\n",
    "preprocessed_data = []\n",
    "\n",
    "# Iterate over the subfolders\n",
    "for folder in ['shanghai', 'guangzhou']:\n",
    "    folder_path = os.path.join(dataset_path, folder)\n",
    "    \n",
    "    # Iterate over the CSV files in the subfolder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            \n",
    "            # Load the CSV file into a DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Perform data cleaning and filtering\n",
    "            df = df[['Location Area Code(LAC)', 'Cell Identity(CI)', 'Start Time', 'End Time',\n",
    "                     'Duration', 'Uplink traffic', 'Downlink traffic', 'Radio Access Type(RAT)',\n",
    "                     'Source IP', 'Destination IP', 'URL', 'User Agent']]\n",
    "            \n",
    "            # Apply any additional preprocessing steps if required\n",
    "            \n",
    "            # Append the preprocessed data to the list\n",
    "            preprocessed_data.append(df)\n",
    "\n",
    "# Concatenate all the preprocessed dataframes into a single dataframe\n",
    "preprocessed_df = pd.concat(preprocessed_data, ignore_index=True)\n",
    "\n",
    "# Analyze content popularity\n",
    "content_popularity = preprocessed_df['URL'].value_counts()\n",
    "\n",
    "# Design caching policy using LRU algorithm\n",
    "cache_size = 1000  # Specify the cache size\n",
    "\n",
    "# Sort the content popularity in descending order\n",
    "sorted_content_popularity = content_popularity.sort_values(ascending=False)\n",
    "\n",
    "# Define an ordered dictionary class that handles NoneType values properly\n",
    "class CostAwareLRUCache(OrderedDict):\n",
    "    def __init__(self, cache_size):\n",
    "        self.cache_size = cache_size\n",
    "        self.total_cost = 0\n",
    "        super().__init__()\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        if key not in self:\n",
    "            OrderedDict.__setitem__(self, key, [value, 0])\n",
    "        self[key][1] += 1\n",
    "        self.move_to_end(key)\n",
    "        self.total_cost += 1\n",
    "\n",
    "    def evict(self):\n",
    "        if len(self) >= self.cache_size:\n",
    "            min_cost_content = min(self, key=lambda x: self.get_content_cost(x))\n",
    "            del self[min_cost_content]\n",
    "            self.total_cost -= 1\n",
    "\n",
    "    def get_content_cost(self, content):\n",
    "        return self[content][1] / self.total_cost if self.total_cost > 0 else 0\n",
    "\n",
    "    def get_cache_cost(self):\n",
    "        return len(self) / self.cache_size if self.cache_size > 0 else 0\n",
    "\n",
    "# Initialize the caches with their respective cache replacement algorithms\n",
    "lru_cache = OrderedDict()\n",
    "car_cache = CostAwareLRUCache(cache_size)\n",
    "random_cache = OrderedDict()\n",
    "\n",
    "# Function to check if a content is in the cache\n",
    "def in_cache(cache, content):\n",
    "    return content in cache\n",
    "\n",
    "# Function to access a content (update the order for LRU and CAR, no need for Random)\n",
    "def access_content(cache, content):\n",
    "    if in_cache(cache, content):\n",
    "        if cache is not random_cache:\n",
    "            # Move the accessed content to the end to update its recency or cost\n",
    "            cache.move_to_end(content)\n",
    "\n",
    "# Function to add a content to the cache (evict the content based on the cache replacement policy)\n",
    "def add_content_to_cache(cache, content, replacement_algo):\n",
    "    if not in_cache(cache, content):\n",
    "        if len(cache) >= cache_size:\n",
    "            if replacement_algo == 'LRU':\n",
    "                # Remove the least recently used content\n",
    "                cache.popitem(last=False)\n",
    "            elif replacement_algo == 'CAR':\n",
    "                # Evict content based on the CAR replacement policy\n",
    "                cache.evict()\n",
    "            elif replacement_algo == 'Random':\n",
    "                # Remove a random content\n",
    "                random_content = random.choice(list(cache.keys()))\n",
    "                del cache[random_content]\n",
    "        cache[content] = None\n",
    "        # Move the newly added content to the end to mark it as the most recently used/costly\n",
    "\n",
    "# Define evaluation metrics\n",
    "total_requests = len(preprocessed_df)  # Total number of requests\n",
    "cache_hits_lru = 0  # Number of cache hits for LRU\n",
    "cache_hits_car = 0  # Number of cache hits for CAR\n",
    "cache_hits_random = 0  # Number of cache hits for Random\n",
    "traffic_offloaded_lru = 0  # Amount of traffic offloaded to the cache for LRU\n",
    "traffic_offloaded_car = 0  # Amount of traffic offloaded to the cache for CAR\n",
    "traffic_offloaded_random = 0  # Amount of traffic offloaded to the cache for Random\n",
    "\n",
    "# Additional evaluation metrics\n",
    "network_latency = 50  # Example network latency in milliseconds\n",
    "network_bandwidth = 100  # Example network bandwidth in Mbps\n",
    "network_cost_per_byte = 0.001  # Example cost per byte in currency units\n",
    "\n",
    "# Iterate over the dataset\n",
    "for _, row in preprocessed_df.iterrows():\n",
    "    content = row['URL']\n",
    "    \n",
    "    # Check if the content is in the LRU cache\n",
    "    if in_cache(lru_cache, content):\n",
    "        cache_hits_lru += 1\n",
    "    else:\n",
    "        add_content_to_cache(lru_cache, content, 'LRU')\n",
    "        traffic_offloaded_lru += row['Downlink traffic']\n",
    "    \n",
    "    # Check if the content is in the CAR cache\n",
    "    if in_cache(car_cache, content):\n",
    "        cache_hits_car += 1\n",
    "    else:\n",
    "        add_content_to_cache(car_cache, content, 'CAR')\n",
    "        traffic_offloaded_car += row['Downlink traffic']\n",
    "    \n",
    "    # Check if the content is in the Random cache\n",
    "    if in_cache(random_cache, content):\n",
    "        cache_hits_random += 1\n",
    "    else:\n",
    "        add_content_to_cache(random_cache, content, 'Random')\n",
    "        traffic_offloaded_random += row['Downlink traffic']\n",
    "        \n",
    "# Calculate hit ratios\n",
    "hit_ratio_lru = cache_hits_lru / total_requests\n",
    "hit_ratio_car = cache_hits_car / total_requests\n",
    "hit_ratio_random = cache_hits_random / total_requests\n",
    "\n",
    "# Calculate latency reductions (assuming each cache hit avoids a network round trip)\n",
    "latency_reduction_lru = cache_hits_lru / total_requests\n",
    "latency_reduction_car = cache_hits_car / total_requests\n",
    "latency_reduction_random = cache_hits_random / total_requests\n",
    "\n",
    "# Calculate additional evaluation metrics\n",
    "network_latency_reduction_lru = latency_reduction_lru * network_latency\n",
    "network_latency_reduction_car = latency_reduction_car * network_latency\n",
    "network_latency_reduction_random = latency_reduction_random * network_latency\n",
    "\n",
    "network_bandwidth_saved_lru = (1 - traffic_offloaded_lru / total_requests) * network_bandwidth\n",
    "network_bandwidth_saved_car = (1 - traffic_offloaded_car / total_requests) * network_bandwidth\n",
    "network_bandwidth_saved_random = (1 - traffic_offloaded_random / total_requests) * network_bandwidth\n",
    "\n",
    "network_cost_reduction_lru = traffic_offloaded_lru * network_cost_per_byte\n",
    "network_cost_reduction_car = traffic_offloaded_car * network_cost_per_byte\n",
    "network_cost_reduction_random = traffic_offloaded_random * network_cost_per_byte\n",
    "\n",
    "# Additional cache admission policy based on network parameters\n",
    "class NetworkAwareCache(CostAwareLRUCache):\n",
    "    def __init__(self, cache_size, network_latency_threshold, network_bandwidth_threshold):\n",
    "        self.network_latency_threshold = network_latency_threshold\n",
    "        self.network_bandwidth_threshold = network_bandwidth_threshold\n",
    "        super().__init__(cache_size)\n",
    "\n",
    "    def should_cache_content(self, content, network_latency, network_bandwidth):\n",
    "        if network_latency < self.network_latency_threshold and network_bandwidth > self.network_bandwidth_threshold:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "# Adjust the network thresholds and cost assumptions\n",
    "network_latency_threshold = 100  # Adjust the network latency threshold\n",
    "network_bandwidth_threshold = 500  # Adjust the network bandwidth threshold\n",
    "network_cost_per_byte = 0.01  # Adjust the cost per byte assumption\n",
    "\n",
    "# Initialize the network-aware cache with its cache admission policy and network thresholds\n",
    "network_aware_cache = NetworkAwareCache(cache_size, network_latency_threshold, network_bandwidth_threshold)\n",
    "\n",
    "# Iterate over the dataset with network parameters\n",
    "for _, row in preprocessed_df.iterrows():\n",
    "    content = row['URL']\n",
    "    network_latency = random.randint(20, 60)  # Example network latency for the content request\n",
    "    network_bandwidth = random.randint(80, 120)  # Example network bandwidth for the content request\n",
    "    \n",
    "    if network_aware_cache.should_cache_content(content, network_latency, network_bandwidth):\n",
    "        access_content(network_aware_cache, content)\n",
    "    else:\n",
    "        add_content_to_cache(network_aware_cache, content, 'CAR')\n",
    "\n",
    "# Calculate hit ratio, latency reduction, and traffic offloaded for the network-aware cache\n",
    "cache_hits_network_aware = len(network_aware_cache)  # Assuming the cache replacement policy is the same as CAR\n",
    "hit_ratio_network_aware = cache_hits_network_aware / total_requests\n",
    "latency_reduction_network_aware = cache_hits_network_aware / total_requests\n",
    "traffic_offloaded_network_aware = traffic_offloaded_car  # Assuming the cache replacement policy is the same as CAR\n",
    "\n",
    "# Calculate additional evaluation metrics for the network-aware cache\n",
    "network_latency_reduction_network_aware = latency_reduction_network_aware * network_latency\n",
    "network_bandwidth_saved_network_aware = (1 - traffic_offloaded_network_aware / total_requests) * network_bandwidth\n",
    "network_cost_reduction_network_aware = traffic_offloaded_network_aware * network_cost_per_byte\n",
    "\n",
    "# Additional cache admission policy using a dynamic caching algorithm\n",
    "class DynamicCache(OrderedDict):\n",
    "    def __init__(self, cache_size):\n",
    "        self.cache_size = cache_size\n",
    "        super().__init__()\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        if key in self:\n",
    "            del self[key]\n",
    "        elif len(self) >= self.cache_size:\n",
    "            self.popitem(last=False)\n",
    "        OrderedDict.__setitem__(self, key, value)\n",
    "\n",
    "    def should_cache_content(self, content, network_latency, network_bandwidth):\n",
    "        # Implement your dynamic cache admission policy logic here\n",
    "        # This example uses a simple threshold-based policy\n",
    "        if network_latency < 100 and network_bandwidth > 50:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "# Initialize the dynamic cache\n",
    "dynamic_cache = DynamicCache(cache_size)\n",
    "\n",
    "# Iterate over the dataset with network parameters\n",
    "for _, row in preprocessed_df.iterrows():\n",
    "    content = row['URL']\n",
    "    network_latency = random.randint(20, 60)  # Example network latency for the content request\n",
    "    network_bandwidth = random.randint(80, 120)  # Example network bandwidth for the content request\n",
    "    \n",
    "    if dynamic_cache.should_cache_content(content, network_latency, network_bandwidth):\n",
    "        access_content(dynamic_cache, content)\n",
    "    else:\n",
    "        add_content_to_cache(dynamic_cache, content, 'CAR')\n",
    "\n",
    "# Calculate hit ratio, latency reduction, and traffic offloaded for the dynamic cache\n",
    "cache_hits_dynamic = len(dynamic_cache)  # Assuming the cache replacement policy is the same as CAR\n",
    "hit_ratio_dynamic = cache_hits_dynamic / total_requests\n",
    "latency_reduction_dynamic = cache_hits_dynamic / total_requests\n",
    "traffic_offloaded_dynamic = traffic_offloaded_car  # Assuming the cache replacement policy is the same as CAR\n",
    "\n",
    "# Calculate additional evaluation metrics for the dynamic cache\n",
    "network_latency_reduction_dynamic = latency_reduction_dynamic * network_latency\n",
    "network_bandwidth_saved_dynamic = (1 - traffic_offloaded_dynamic / total_requests) * network_bandwidth\n",
    "network_cost_reduction_dynamic = traffic_offloaded_dynamic * network_cost_per_byte\n",
    "\n",
    "# Print the evaluation metrics for each cache replacement algorithm\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(\"LRU - Hit Ratio:\", hit_ratio_lru)\n",
    "print(\"LRU - Latency Reduction:\", latency_reduction_lru)\n",
    "print(\"LRU - Traffic Offloaded:\", traffic_offloaded_lru)\n",
    "print(\"LRU - Network Latency Reduction:\", network_latency_reduction_lru)\n",
    "print(\"LRU - Network Bandwidth Saved:\", network_bandwidth_saved_lru)\n",
    "print(\"LRU - Network Cost Reduction:\", network_cost_reduction_lru)\n",
    "print()\n",
    "print(\"CAR - Hit Ratio:\", hit_ratio_car)\n",
    "print(\"CAR - Latency Reduction:\", latency_reduction_car)\n",
    "print(\"CAR - Traffic Offloaded:\", traffic_offloaded_car)\n",
    "print(\"CAR - Network Latency Reduction:\", network_latency_reduction_car)\n",
    "print(\"CAR - Network Bandwidth Saved:\", network_bandwidth_saved_car)\n",
    "print(\"CAR - Network Cost Reduction:\", network_cost_reduction_car)\n",
    "print()\n",
    "print(\"Random - Hit Ratio:\", hit_ratio_random)\n",
    "print(\"Random - Latency Reduction:\", latency_reduction_random)\n",
    "print(\"Random - Traffic Offloaded:\", traffic_offloaded_random)\n",
    "print(\"Random - Network Latency Reduction:\", network_latency_reduction_random)\n",
    "print(\"Random - Network Bandwidth Saved:\", network_bandwidth_saved_random)\n",
    "print(\"Random - Network Cost Reduction:\", network_cost_reduction_random)\n",
    "print()\n",
    "print(\"Network-Aware - Hit Ratio:\", hit_ratio_network_aware)\n",
    "print(\"Network-Aware - Latency Reduction:\", latency_reduction_network_aware)\n",
    "print(\"Network-Aware - Traffic Offloaded:\", traffic_offloaded_network_aware)\n",
    "print(\"Network-Aware - Network Latency Reduction:\", network_latency_reduction_network_aware)\n",
    "print(\"Network-Aware - Network Bandwidth Saved:\", network_bandwidth_saved_network_aware)\n",
    "print(\"Network-Aware - Network Cost Reduction:\", network_cost_reduction_network_aware)\n",
    "print()\n",
    "print(\"Dynamic - Hit Ratio:\", hit_ratio_dynamic)\n",
    "print(\"Dynamic - Latency Reduction:\", latency_reduction_dynamic)\n",
    "print(\"Dynamic - Traffic Offloaded:\", traffic_offloaded_dynamic)\n",
    "print(\"Dynamic - Network Latency Reduction:\", network_latency_reduction_dynamic)\n",
    "print(\"Dynamic - Network Bandwidth Saved:\", network_bandwidth_saved_dynamic)\n",
    "print(\"Dynamic - Network Cost Reduction:\", network_cost_reduction_dynamic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39872a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Edge Cache - Hit Ratio: 0.024411320994224283\n",
      "Edge Cache - Latency Reduction: 0.024411320994224283\n",
      "Edge Cache - Traffic Offloaded: 6475728621\n"
     ]
    }
   ],
   "source": [
    "# Define the capacity of the edge cache and central cache\n",
    "edge_cache_capacity = 5000  # Specify the capacity of the edge cache\n",
    "central_cache_capacity = 10000  # Specify the capacity of the central cache\n",
    "\n",
    "# Initialize the edge cache and central cache with their respective cache replacement algorithms\n",
    "edge_cache = OrderedDict()\n",
    "central_cache = OrderedDict()\n",
    "\n",
    "# Function to add content to the cache based on the cache level and capacity\n",
    "def add_content_to_cache(cache, content, replacement_algo, capacity):\n",
    "    if not in_cache(cache, content):\n",
    "        if len(cache) >= capacity:\n",
    "            if replacement_algo == 'LRU':\n",
    "                # Remove the least recently used content\n",
    "                cache.popitem(last=False)\n",
    "            elif replacement_algo == 'CAR':\n",
    "                # Evict content based on the CAR replacement policy\n",
    "                cache.evict()\n",
    "            elif replacement_algo == 'Random':\n",
    "                # Remove a random content\n",
    "                random_content = random.choice(list(cache.keys()))\n",
    "                del cache[random_content]\n",
    "        cache[content] = None\n",
    "        # Move the newly added content to the end to mark it as the most recently used\n",
    "\n",
    "# Iterate over the dataset\n",
    "for _, row in preprocessed_df.iterrows():\n",
    "    content = row['URL']\n",
    "    \n",
    "    # Check if the content is in the edge cache\n",
    "    if in_cache(edge_cache, content):\n",
    "        cache_hits_lru += 1\n",
    "    elif in_cache(central_cache, content):\n",
    "        # Move the content from the central cache to the edge cache\n",
    "        edge_cache[content] = central_cache.pop(content)\n",
    "        cache_hits_lru += 1\n",
    "    else:\n",
    "        add_content_to_cache(edge_cache, content, 'LRU', edge_cache_capacity)\n",
    "        if len(edge_cache) > edge_cache_capacity:\n",
    "            add_content_to_cache(central_cache, content, 'CAR', central_cache_capacity)\n",
    "        traffic_offloaded_lru += row['Downlink traffic']\n",
    "\n",
    "# Calculate hit ratio, latency reduction, and traffic offloaded for the edge cache\n",
    "cache_hits_edge = len(edge_cache)\n",
    "hit_ratio_edge = cache_hits_edge / total_requests\n",
    "latency_reduction_edge = cache_hits_edge / total_requests\n",
    "traffic_offloaded_edge = traffic_offloaded_lru\n",
    "\n",
    "# Print the evaluation metrics for each cache level\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(\"Edge Cache - Hit Ratio:\", hit_ratio_edge)\n",
    "print(\"Edge Cache - Latency Reduction:\", latency_reduction_edge)\n",
    "print(\"Edge Cache - Traffic Offloaded:\", traffic_offloaded_edge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2b7d8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
